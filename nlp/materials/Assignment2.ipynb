{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thefr33radical/courses/blob/master/nlp/materials/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USeJ8W5LJFPI",
        "colab_type": "code",
        "outputId": "0f692f1a-8aff-41de-97c7-9b7dda6a67e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWDv3sGQKnuN",
        "colab_type": "code",
        "outputId": "8c1be67f-d500-419d-c3c5-437f8ab3cc0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#!`--NotebookApp.iopub_data_rate_limit`.\n",
        "#!pip3 install bs4\n",
        "#!pip3 install gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: --NotebookApp.iopub_data_rate_limit: command not found\n",
            "/bin/bash: line 0: .: filename argument required\n",
            ".: usage: . filename [arguments]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmQZswwEK7tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.parsing.preprocessing import stem_text\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from gensim.parsing.preprocessing import strip_non_alphanum\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import gensim\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "import math\n",
        "\n",
        "class Salient(object):  \n",
        "  \n",
        "  def generate_data(self,soup):\n",
        "    sent=soup.find_all(\"context\")\n",
        "    for i in sent:      \n",
        "      yield i   \n",
        "  \n",
        "  def collocation(self,context_str):\n",
        "    collc=[]\n",
        "    cvocab=[]\n",
        "    \n",
        "    with open(path+\"wsd_data.xml\",\"rb\") as f:\n",
        "      data=f.read()\n",
        "\n",
        "      soup=BeautifulSoup(data,'html.parser') \n",
        "      count =0     \n",
        "      c=1\n",
        "      count=0\n",
        "      head_str=[]\n",
        "      for s in (self.generate_data(soup)):\n",
        "        try:\n",
        "          \n",
        "          head_str=re.findall(\"<head>(.*?)</head>\", str(s))\n",
        "          if len(head_str)<1 or head_str[0]==\"\":\n",
        "            continue\n",
        "          \n",
        "          temp=s.text\n",
        "          if temp==\"\":\n",
        "            continue\n",
        "\n",
        "          temp=strip_non_alphanum(temp)\n",
        "          #temp =remove_stopwords(temp)\n",
        "          temp=str.lower(temp)\n",
        "          #temp =stem_text(temp)\n",
        "\n",
        "          sent_list= temp.split(\" \")          \n",
        "\n",
        "          index=sent_list.index(head_str[0])\n",
        "                  \n",
        "          v_sent= sent_list[index-1:index+1]\n",
        "          \n",
        "          for v in v_sent:   \n",
        "            \n",
        "            if v not in cvocab and len(v)>3:\n",
        "              cvocab.append(c) \n",
        "              c+=1\n",
        "\n",
        "          head_str=[]\n",
        "        except:\n",
        "          pass\n",
        "    print(cvocab)    \n",
        "    with open(path+\"cvocabs.csv\",\"w+\") as f:\n",
        "      wr=csv.DictWriter(f,[\"cvocab\"])\n",
        "      \n",
        "      for i in cvocab:\n",
        "        wr.writerow({'cvocab': i})   \n",
        "      \n",
        "  \n",
        "  def ReadData(self,path):\n",
        "    \"\"\"\n",
        "    Function to read data and form vocabulary\n",
        "    \"\"\"\n",
        "    docs=[]\n",
        "    vocab=[]\n",
        "    \n",
        "    with open(path+\"wsd_data.xml\",\"rb\") as f:\n",
        "      data=f.read()\n",
        "\n",
        "      soup=BeautifulSoup(data,'html.parser') \n",
        "      count =0     \n",
        "\n",
        "      count=0\n",
        "      head_str=[]\n",
        "      for s in (self.generate_data(soup)):\n",
        "        try:\n",
        "          \n",
        "          head_str=re.findall(\"<head>(.*?)</head>\", str(s))\n",
        "          if len(head_str)<1 or head_str[0]==\"\":\n",
        "            continue\n",
        "          \n",
        "          temp=s.text\n",
        "          if temp==\"\":\n",
        "            continue\n",
        "\n",
        "          temp=strip_non_alphanum(temp)\n",
        "          #temp =remove_stopwords(temp)\n",
        "          temp=str.lower(temp)\n",
        "          #temp =stem_text(temp)\n",
        "\n",
        "          sent_list= temp.split(\" \")          \n",
        "\n",
        "          index=sent_list.index(head_str[0])\n",
        "                  \n",
        "          v_sent= sent_list[index-1:index+1]\n",
        "          for v in v_sent:            \n",
        "            if v not in vocab and len(v)>3:\n",
        "              vocab.append(v)          \n",
        "\n",
        "          docs.append(temp)\n",
        "          head_str=[]\n",
        "        except:\n",
        "          pass\n",
        "    \n",
        "    with open(path+\"docs.csv\",\"w+\") as f:\n",
        "      wr=csv.DictWriter(f,[\"document_name\"])\n",
        "      for i in docs:\n",
        "        wr.writerow({'document_name': i})\n",
        "        \n",
        "    with open(path+\"vocabs.csv\",\"w+\") as f:\n",
        "      wr=csv.DictWriter(f,[\"vocab\"])\n",
        "      for i in vocab:\n",
        "        wr.writerow({'vocab': i}) \n",
        "        \n",
        "   \n",
        "  def TF(self,path):\n",
        "    \"\"\"\n",
        "    Function to compute Term Frequency matrix\n",
        "    \n",
        "    \"\"\"\n",
        "    docs=pd.read_csv(path+\"docs.csv\")\n",
        "    docs.columns=[\"_DOCS\"]\n",
        "    docs.dropna(subset=['_DOCS'], how='any', inplace = True)\n",
        "\n",
        "    #print(docs)\n",
        "    \n",
        "    v=pd.read_csv(path+\"vocabs.csv\")\n",
        "    vocab=list(v.iloc[:,0])\n",
        "    vocab=vocab[:500]\n",
        "    TF=pd.DataFrame()\n",
        "    for i in vocab[:500]:\n",
        "      TF[i]=[0]*len(docs)\n",
        "      \n",
        "    TF[\"_DOCS\"]=list(docs[\"_DOCS\"])\n",
        "    #TF=TF.dropna(subset=['_DOCS'], how='any', inplace = True)\n",
        "    #print(TF)\n",
        "   \n",
        "    total_docs=len(docs)\n",
        "    print(total_docs,len(TF.columns))\n",
        "    \n",
        "    \n",
        "    for word in vocab:\n",
        "      count=0\n",
        "      total_docs=0\n",
        "      \n",
        "      for d in range(len(TF[\"_DOCS\"])):\n",
        "             \n",
        "        try:\n",
        "          \n",
        "          count=TF.loc[d,\"_DOCS\"].count(word )\n",
        "          total_len=len(TF.loc[d,\"_DOCS\"])\n",
        "          TF.loc[d,word]=float(count/total_len)\n",
        "          if count>0:\n",
        "            print(stem_text(word), stem_text(TF.loc[d,\"_DOCS\"]))\n",
        "            print(TF.loc[d,word],word,TF.loc[d,\"_DOCS\"])          \n",
        "        except Exception as e:\n",
        "          print(word,TF.loc[d,\"_DOCS\"],e)\n",
        "          pass\n",
        "        \n",
        "    TF.to_csv(path+\"TF.csv\")\n",
        "      \n",
        "      \n",
        "  def IDF(self,path):\n",
        "    docs=pd.read_csv(path+\"docs.csv\")\n",
        "    v=pd.read_csv(path+\"vocabs.csv\")\n",
        "    vc=v.iloc[:,0]\n",
        "    vectorizer = TfidfVectorizer(vocabulary=vc)\n",
        "    X = vectorizer.fit_transform(docs.iloc[:,0])\n",
        "    print(X.shape)\n",
        "    newX=X.toarray()\n",
        "    new_data=pd.DataFrame(newX)    \n",
        "    new_data.to_csv(path+\"IDF.csv\")\n",
        "    \n",
        "    pca = PCA(n_components=10)\n",
        "    result=pca.fit_transform(newX) \n",
        "    print(pca.explained_variance_ratio_)\n",
        "    \n",
        "    \n",
        "obj=Salient()\n",
        "\n",
        "path='/content/drive/My Drive/'\n",
        "#docs=obj.ReadData('/content/drive/My Drive/')\n",
        "#obj.collocation(path)\n",
        "#obj.TF(path)\n",
        "obj.IDF(path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}